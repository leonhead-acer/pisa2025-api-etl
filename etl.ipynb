{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare / Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Maple Data and Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "from etl.prepare import *\n",
    "\n",
    "with open('./config.yaml', 'r') as file:\n",
    "    config_data = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redshift_connector\n",
    "from etl.postgresqlschemareader import *\n",
    "\n",
    "with redshift_connector.connect(\n",
    "    host=config_data['ams']['host'],\n",
    "    database=config_data['ams']['database'],\n",
    "    user=config_data['ams']['username'],\n",
    "    password=config_data['ams']['password'],\n",
    "    timeout=999999,\n",
    "    port=5439\n",
    ") as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"\"\"SELECT table_schema, table_name\n",
    "                      FROM information_schema.tables\n",
    "                      WHERE table_schema != 'pg_catalog'\n",
    "                      AND table_schema != 'information_schema'\n",
    "                      AND table_type='BASE TABLE'\n",
    "                      ORDER BY table_schema, table_name\"\"\")\n",
    "\n",
    "        tables = cur.fetchall()\n",
    "        print(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from etl.extract import *\n",
    "from etl.transform import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_num = '1'\n",
    "\n",
    "with postgresql_conn(params = config_data['postgresql_prod']) as conn:\n",
    "    conn.autocommit = True\n",
    "    with conn.cursor() as cur:\n",
    "        # student_participants = extract_student_participants(filepath = '../maple-s3/participants_post.xlsx')\n",
    "        cur.execute(\"SELECT * FROM maple.isot_table\")\n",
    "        isot_table = pd.DataFrame(cur.fetchall()).drop_duplicates('isocntcd',keep = 'last')\n",
    "        isot_table.at[26,'isoalpha3'] = 'BEL'\n",
    "        isot_table.at[241,'isoalpha3'] = 'TAP'\n",
    "        cur.execute(\"SELECT * FROM maple.maple_student_post_val\")\n",
    "        student_participants_post = pd.DataFrame(cur.fetchall())\n",
    "        student_participants_post['username'] = student_participants_post['username'].astype(str)\n",
    "        student_participants_post['isocntcd'] = student_participants_post['username'].str.slice(1,4)\n",
    "\n",
    "        countries_all = isot_table.isoalpha3.unique()\n",
    "        countries_now = list(student_participants_post.loc[student_participants_post['batch'] == batch_num,:].isoalpha3.unique())\n",
    "        countries_post = list(student_participants_post.loc[student_participants_post['batch'] != batch_num,:].isoalpha3.unique())\n",
    "        countries_pre_init = list(set(countries_all) - set(countries_now) - set(countries_post))\n",
    "\n",
    "        student_participants_pre = pd.read_excel('../maple-s3/participants_with_entity.xlsx').drop_duplicates(['username'],keep = 'last')\n",
    "        student_participants_pre['username'] = student_participants_pre['username'].astype(str)\n",
    "        student_participants_pre['isocntcd'] = student_participants_pre['username'].str.slice(1,4)\n",
    "        student_participants_pre = student_participants_pre.loc[student_participants_pre['isoalpha3'].isin(countries_pre_init)].drop_duplicates(subset = ['username'],keep = 'last')\n",
    "        student_participants_pre['login'] = student_participants_pre['username']\n",
    "        student_participants_pre = student_participants_pre.rename({'testAttendance':'test_attendance','questionnaireAttendance': 'questionnaire_attendance'},axis = 1)\n",
    "        countries_pre = list(set(student_participants_pre.isoalpha3.unique()) - set(['GBR']))\n",
    "\n",
    "        student_participants = pd.concat(\n",
    "            [\n",
    "                student_participants_pre,\n",
    "                student_participants_post\n",
    "            ],\n",
    "            axis = 0\n",
    "        )\n",
    "\n",
    "        student_participants['schid'] = student_participants['username'].str.slice(4,8).str.lstrip('0').astype(int)\n",
    "\n",
    "        conditions = [\n",
    "            (student_participants['schid'] <= 118) & (student_participants['isoalpha3'] == 'BEL'),\n",
    "            (student_participants['schid'] >= 119) & (student_participants['isoalpha3'] == 'BEL')\n",
    "        ]\n",
    "\n",
    "        codes = [\n",
    "            'QBL',\n",
    "            'QBR'\n",
    "        ]\n",
    "\n",
    "        student_participants['isoalpha3_new'] = np.select(conditions,codes,student_participants['isoalpha3'])\n",
    "        student_participants = student_participants.drop(columns = ['isoalpha3']).rename({'isoalpha3_new':'isoalpha3'},axis = 1)\n",
    "        \n",
    "        nc_dat_now = isot_table.loc[isot_table['isoalpha3'].isin(countries_now)].assign(process='now')\n",
    "        nc_dat_post = isot_table.loc[isot_table['isoalpha3'].isin(countries_post)].assign(process='post')\n",
    "        nc_dat_pre = isot_table.loc[isot_table['isoalpha3'].isin(countries_pre)].assign(process='pre')  \n",
    "        nc_dat = pd.concat(\n",
    "            [\n",
    "                nc_dat_post,\n",
    "                nc_dat_now,\n",
    "                nc_dat_pre\n",
    "            ],\n",
    "            axis = 0\n",
    "        )\n",
    "        # nc_dat = extract_country_codes(filepath = './data/maple-s3/ISOT_table.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_dat = pd.concat(\n",
    "    [\n",
    "        nc_dat.loc[nc_dat['isoalpha3'] == \"ESP\",:],\n",
    "        nc_dat.loc[nc_dat['isoalpha3'] != \"ESP\",:]\n",
    "    ],\n",
    "    axis = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'etl.transform' from 'd:\\\\Users\\\\leon.head\\\\Documents\\\\pisa2025-api-etl\\\\etl\\\\transform.py'>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, importlib\n",
    "importlib.reload(sys.modules['etl.transform'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting codebook data from sheet FLA_Reading_CQ\n",
      "Extracting codebook data from sheet FLA_Listening_CQ\n",
      "Codebook created for FLA\n",
      "Processing data for: ESP\n",
      "Step 1: rows = 3198 & columns = 9\n",
      "Step 2: rows = 3198 & columns = 33\n",
      "Step 3: rows = 74552 & columns = 8\n",
      "Step 4: rows = 74552 & columns = 23\n",
      "Step 5: rows = 218790 & columns = 22\n",
      "Step 6: rows = 221399 & columns = 49\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_ESP.json: 503.49 seconds\n",
      "Processing data for: AUT\n",
      "Processing data for: BRN\n",
      "Processing data for: QCY\n",
      "Step 1: rows = 898 & columns = 9\n",
      "Step 2: rows = 898 & columns = 33\n",
      "Step 3: rows = 20965 & columns = 8\n",
      "Step 4: rows = 20965 & columns = 23\n",
      "Step 5: rows = 61523 & columns = 22\n",
      "Step 6: rows = 62245 & columns = 49\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_QCY.json: 68.97 seconds\n",
      "Processing data for: DEU\n",
      "Step 1: rows = 1240 & columns = 9\n",
      "Step 2: rows = 1240 & columns = 33\n",
      "Step 3: rows = 28950 & columns = 8\n",
      "Step 4: rows = 28950 & columns = 23\n",
      "Step 5: rows = 84868 & columns = 22\n",
      "Step 6: rows = 85921 & columns = 49\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_DEU.json: 177.24 seconds\n",
      "Processing data for: DNK\n",
      "Step 1: rows = 885 & columns = 9\n",
      "Step 2: rows = 885 & columns = 33\n",
      "Step 3: rows = 20709 & columns = 8\n",
      "Step 4: rows = 20709 & columns = 23\n",
      "Step 5: rows = 60824 & columns = 22\n",
      "Step 6: rows = 61569 & columns = 49\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_DNK.json: 68.50 seconds\n",
      "Processing data for: QUK\n",
      "Processing data for: QSC\n",
      "Processing data for: HRV\n",
      "Step 1: rows = 1569 & columns = 9\n",
      "Step 2: rows = 1569 & columns = 33\n",
      "Step 3: rows = 36644 & columns = 8\n",
      "Step 4: rows = 36644 & columns = 23\n",
      "Step 5: rows = 107138 & columns = 22\n",
      "Step 6: rows = 108468 & columns = 49\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_HRV.json: 153.35 seconds\n",
      "Processing data for: IDN\n",
      "Processing data for: IRL\n",
      "Processing data for: ITA\n",
      "Step 1: rows = 1288 & columns = 9\n",
      "Step 2: rows = 1288 & columns = 33\n",
      "Step 3: rows = 30118 & columns = 8\n",
      "Step 4: rows = 30118 & columns = 23\n",
      "Step 5: rows = 88284 & columns = 22\n",
      "Step 6: rows = 89321 & columns = 49\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_ITA.json: 127.95 seconds\n",
      "Processing data for: LTU\n",
      "Processing data for: MAC\n",
      "Processing data for: MNE\n",
      "Processing data for: MNG\n",
      "Processing data for: NLD\n",
      "Step 1: rows = 642 & columns = 9\n",
      "Step 2: rows = 642 & columns = 33\n",
      "Step 3: rows = 15023 & columns = 8\n",
      "Step 4: rows = 15023 & columns = 23\n",
      "Step 5: rows = 43594 & columns = 22\n",
      "Step 6: rows = 44147 & columns = 49\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_NLD.json: 53.31 seconds\n",
      "Processing data for: PRT\n",
      "Step 1: rows = 1458 & columns = 9\n",
      "Step 2: rows = 1458 & columns = 33\n",
      "Step 3: rows = 33934 & columns = 8\n",
      "Step 4: rows = 33934 & columns = 23\n",
      "Step 5: rows = 99496 & columns = 22\n",
      "Step 6: rows = 100694 & columns = 49\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_PRT.json: 219.44 seconds\n",
      "Processing data for: QAT\n",
      "Step 1: rows = 1342 & columns = 9\n",
      "Step 2: rows = 1342 & columns = 33\n",
      "Step 3: rows = 31301 & columns = 8\n",
      "Step 4: rows = 31301 & columns = 23\n",
      "Step 5: rows = 91777 & columns = 22\n",
      "Step 6: rows = 92842 & columns = 49\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_QAT.json: 120.39 seconds\n",
      "Processing data for: SAU\n",
      "Processing data for: SGP\n",
      "Processing data for: SRB\n",
      "Processing data for: SVN\n",
      "Processing data for: SWE\n",
      "Step 1: rows = 1303 & columns = 9\n",
      "Step 2: rows = 1303 & columns = 33\n",
      "Step 3: rows = 30426 & columns = 8\n",
      "Step 4: rows = 30426 & columns = 23\n",
      "Step 5: rows = 89276 & columns = 22\n",
      "Step 6: rows = 90330 & columns = 49\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_SWE.json: 110.17 seconds\n",
      "Processing data for: TUR\n",
      "Processing data for: URY\n",
      "Processing data for: ALB\n",
      "Processing data for: ARM\n",
      "Processing data for: AUS\n",
      "Processing data for: BEL\n",
      "Step 1: rows = 1062 & columns = 9\n",
      "Step 2: rows = 1062 & columns = 33\n",
      "Step 3: rows = 24768 & columns = 8\n",
      "Step 4: rows = 24768 & columns = 23\n",
      "Step 5: rows = 72746 & columns = 22\n",
      "Step 6: rows = 73631 & columns = 49\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_BEL.json: 191.39 seconds\n",
      "Processing data for: BGR\n",
      "Step 1: rows = 1636 & columns = 9\n",
      "Step 2: rows = 1636 & columns = 33\n",
      "Step 3: rows = 38149 & columns = 8\n",
      "Step 4: rows = 38149 & columns = 23\n",
      "Step 5: rows = 111849 & columns = 22\n",
      "Step 6: rows = 113199 & columns = 49\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_BGR.json: 129.52 seconds\n",
      "Processing data for: BRA\n",
      "Processing data for: CAN\n",
      "Processing data for: CHE\n",
      "Processing data for: CHL\n",
      "Processing data for: COL\n",
      "Step 1: rows = 1222 & columns = 9\n",
      "Step 2: rows = 1222 & columns = 33\n",
      "Step 3: rows = 28663 & columns = 8\n",
      "Step 4: rows = 28663 & columns = 23\n",
      "Step 5: rows = 83567 & columns = 22\n",
      "Step 6: rows = 84592 & columns = 49\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_COL.json: 254.51 seconds\n",
      "Processing data for: CRI\n",
      "Processing data for: CZE\n",
      "Step 1: rows = 1306 & columns = 9\n",
      "Step 2: rows = 1306 & columns = 33\n",
      "Step 3: rows = 30408 & columns = 8\n",
      "Step 4: rows = 30408 & columns = 23\n",
      "Step 5: rows = 89162 & columns = 22\n",
      "Step 6: rows = 90263 & columns = 49\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_CZE.json: 255.07 seconds\n",
      "Processing data for: ECU\n",
      "Processing data for: EST\n",
      "Processing data for: FIN\n",
      "Step 1: rows = 1236 & columns = 9\n",
      "Step 2: rows = 1236 & columns = 33\n",
      "Step 3: rows = 28820 & columns = 8\n",
      "Step 4: rows = 28820 & columns = 23\n",
      "Step 5: rows = 84324 & columns = 22\n",
      "Step 6: rows = 85331 & columns = 49\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_FIN.json: 184.65 seconds\n",
      "Processing data for: FRA\n",
      "Step 1: rows = 1184 & columns = 9\n",
      "Step 2: rows = 1184 & columns = 33\n",
      "Step 3: rows = 27671 & columns = 8\n",
      "Step 4: rows = 27671 & columns = 23\n",
      "Step 5: rows = 81216 & columns = 22\n",
      "Step 6: rows = 82198 & columns = 49\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_FRA.json: 258.32 seconds\n",
      "Processing data for: GEO\n",
      "Processing data for: GRC\n",
      "Step 1: rows = 1216 & columns = 9\n",
      "Step 2: rows = 1216 & columns = 33\n",
      "Step 3: rows = 28339 & columns = 8\n",
      "Step 4: rows = 28339 & columns = 23\n",
      "Step 5: rows = 82869 & columns = 22\n",
      "Step 6: rows = 83898 & columns = 49\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_GRC.json: 98.92 seconds\n",
      "Processing data for: GTM\n",
      "Processing data for: HKG\n",
      "Processing data for: HUN\n",
      "Processing data for: ISL\n",
      "Processing data for: ISR\n",
      "Step 1: rows = 807 & columns = 9\n",
      "Step 2: rows = 807 & columns = 33\n",
      "Step 3: rows = 18919 & columns = 8\n",
      "Step 4: rows = 18919 & columns = 23\n",
      "Step 5: rows = 55415 & columns = 22\n",
      "Step 6: rows = 56086 & columns = 49\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_ISR.json: 166.95 seconds\n",
      "Processing data for: JOR\n",
      "Processing data for: JPN\n",
      "Processing data for: KAZ\n",
      "Processing data for: KEN\n",
      "Processing data for: KHM\n",
      "Processing data for: KOR\n",
      "Processing data for: LBN\n",
      "Processing data for: LUX\n",
      "Processing data for: LVA\n",
      "Processing data for: MAR\n",
      "Processing data for: MDA\n",
      "Processing data for: MLT\n",
      "Processing data for: MUS\n",
      "Processing data for: MYS\n",
      "Processing data for: NOR\n",
      "Processing data for: NZL\n",
      "Processing data for: PHL\n",
      "Processing data for: POL\n",
      "Processing data for: PRY\n",
      "Processing data for: PSE\n",
      "Processing data for: ROU\n",
      "Step 1: rows = 1380 & columns = 9\n",
      "Step 2: rows = 1380 & columns = 33\n",
      "Step 3: rows = 32202 & columns = 8\n",
      "Step 4: rows = 32202 & columns = 23\n",
      "Step 5: rows = 94452 & columns = 22\n",
      "Step 6: rows = 95569 & columns = 49\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_ROU.json: 314.70 seconds\n",
      "Processing data for: RWA\n",
      "Processing data for: SVK\n",
      "Processing data for: THA\n",
      "Processing data for: TJK\n",
      "Processing data for: TAP\n",
      "Step 1: rows = 1272 & columns = 9\n",
      "Step 2: rows = 1272 & columns = 33\n",
      "Step 3: rows = 29740 & columns = 8\n",
      "Step 4: rows = 29740 & columns = 23\n",
      "Step 5: rows = 87149 & columns = 22\n",
      "Step 6: rows = 88186 & columns = 49\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_TAP.json: 232.01 seconds\n",
      "Processing data for: UKR\n",
      "Step 1: rows = 1176 & columns = 9\n",
      "Step 2: rows = 1176 & columns = 33\n",
      "Step 3: rows = 27478 & columns = 8\n",
      "Step 4: rows = 27478 & columns = 23\n",
      "Step 5: rows = 80685 & columns = 22\n",
      "Step 6: rows = 81659 & columns = 49\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_UKR.json: 101.43 seconds\n",
      "Processing data for: USA\n",
      "Processing data for: UZB\n",
      "Processing data for: VNM\n"
     ]
    }
   ],
   "source": [
    "import glob, time\n",
    "\n",
    "count = 0\n",
    "domain_all = [\"FLA\"]\n",
    "\n",
    "for domain in domain_all:\n",
    "    cbk = create_codebook(domain = domain)\n",
    "    \n",
    "    if(domain == 'FLA'):\n",
    "        gap_vars = True\n",
    "\n",
    "    for idr,row in nc_dat.iterrows():\n",
    "        start_time = time.time()\n",
    "        country_print = str(row['isoalpha3'])\n",
    "        print(f\"Processing data for: {country_print}\")\n",
    "\n",
    "        # extract_json(domain = domain, nc_dat = row ,overwrite = False, con = postgresql_conn(params = config_data['postgresql_prod']))\n",
    "\n",
    "        filepath = f\"./data/db/{domain.lower()}/{domain}_{country_print}.json\"\n",
    "\n",
    "        if(os.path.isfile(filepath)):\n",
    "\n",
    "            filepath_csv = f\"./data/db/{domain.lower()}/{domain}_{country_print}.csv\"\n",
    "\n",
    "            df = read_json_file(filepath)\n",
    "            print(\"Step 1: rows = \" + str(df.shape[0]) + ' & columns = ' + str(df.shape[1]))\n",
    "            \n",
    "            df1 = explode_raw_data(df = df)\n",
    "            print(\"Step 2: rows = \" + str(df1.shape[0]) + ' & columns = ' + str(df1.shape[1]))\n",
    "\n",
    "            df3 = explode_items(df1)\n",
    "            print(\"Step 3: rows = \" + str(df3.shape[0]) + ' & columns = ' + str(df3.shape[1]))\n",
    "\n",
    "            df4 = explode_values(df3)\n",
    "            df4 = rename_variables(df4, domain = domain)\n",
    "            df4 = check_duplicates(df4)\n",
    "            df4 = replace_blank_json(df4)\n",
    "            print(\"Step 4: rows = \" + str(df4.shape[0]) + ' & columns = ' + str(df4.shape[1]))\n",
    "\n",
    "            df6 = explode_responses(df4, domain = domain)\n",
    "            if(domain == 'FLA'):\n",
    "                df6 = fla_recode_FLALDTB1002(df6)\n",
    "            if(gap_vars):\n",
    "                df6 = gap_recode(df6,cbk)\n",
    "            print(\"Step 5: rows = \" + str(df6.shape[0]) + ' & columns = ' + str(df6.shape[1]))\n",
    "\n",
    "            df8 = merge_cbk_status(df6,cbk,domain = 'FLA')\n",
    "            df8 = time_var_recode(df8)\n",
    "            df8 = score_resp_recode(df8,domain = 'FLA') \n",
    "            df8 = trailing_missing(df8,cbk=cbk)\n",
    "            df8 = cmc_item_create(df8,cbk=cbk, domain = 'FLA')\n",
    "            df8.to_csv(filepath_csv)\n",
    "            df9 = merge_participant_info(df8,student_participants=student_participants)\n",
    "            print(\"Step 6: rows = \" + str(df9.shape[0]) + ' & columns = ' + str(df9.shape[1]))\n",
    "\n",
    "            df_resp_check = sql_query_ge(nc_dat = row,cbk = cbk,con = postgresql_conn(params = config_data['postgresql_prod']))\n",
    "\n",
    "            if(count == 0):\n",
    "                df_long = df9\n",
    "                df_long_check = df_resp_check\n",
    "            else:\n",
    "                df_long = pd.concat([df_long,df9],axis = 0)\n",
    "                df_long_check = pd.concat([df_long_check,df_resp_check],axis = 0)\n",
    "            \n",
    "            count =+ 1\n",
    "\n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "            print(f\"Time taken for {filepath}: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "        # df9.export_to_postgresql()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>login</th>\n",
       "      <th>last_update_date</th>\n",
       "      <th>testQtiLabel</th>\n",
       "      <th>sessionStartTime</th>\n",
       "      <th>sessionEndTime</th>\n",
       "      <th>language</th>\n",
       "      <th>unit_id</th>\n",
       "      <th>itemId</th>\n",
       "      <th>score</th>\n",
       "      <th>...</th>\n",
       "      <th>dob_mm</th>\n",
       "      <th>dob_yy</th>\n",
       "      <th>sen</th>\n",
       "      <th>mpop1</th>\n",
       "      <th>ppart1</th>\n",
       "      <th>isoalpha3</th>\n",
       "      <th>isoname</th>\n",
       "      <th>isocntcd</th>\n",
       "      <th>test_attendance</th>\n",
       "      <th>questionnaire_attendance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>17240191032</td>\n",
       "      <td>1.720307e+12</td>\n",
       "      <td>FLA-R-M4-FLA-R-L1-FLA-R-H1</td>\n",
       "      <td>2024-05-14 17:40:25</td>\n",
       "      <td>2024-05-14 18:21:43</td>\n",
       "      <td>en-ZZ</td>\n",
       "      <td>FLARDGSA2004</td>\n",
       "      <td>cluster1-FLAR19-item-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ESP</td>\n",
       "      <td>Spain</td>\n",
       "      <td>724</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>17240297040</td>\n",
       "      <td>1.720315e+12</td>\n",
       "      <td>FLA-R-M4-FLA-R-L1-FLA-R-H1</td>\n",
       "      <td>2024-04-18 17:32:31</td>\n",
       "      <td>2024-04-18 18:00:44</td>\n",
       "      <td>en-ZZ</td>\n",
       "      <td>FLARDGSA2004</td>\n",
       "      <td>cluster1-FLAR19-item-1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ESP</td>\n",
       "      <td>Spain</td>\n",
       "      <td>724</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>17240059030</td>\n",
       "      <td>1.720318e+12</td>\n",
       "      <td>FLA-R-M4-FLA-R-L1-FLA-R-H1</td>\n",
       "      <td>2024-04-19 17:48:35</td>\n",
       "      <td>2024-04-19 18:33:31</td>\n",
       "      <td>en-ZZ</td>\n",
       "      <td>FLARDGSA2004</td>\n",
       "      <td>cluster1-FLAR19-item-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ESP</td>\n",
       "      <td>Spain</td>\n",
       "      <td>724</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>17240291033</td>\n",
       "      <td>1.720319e+12</td>\n",
       "      <td>FLA-R-M4-FLA-R-L1-FLA-R-H1</td>\n",
       "      <td>2024-05-16 16:53:43</td>\n",
       "      <td>2024-05-16 17:09:31</td>\n",
       "      <td>en-ZZ</td>\n",
       "      <td>FLARDGSA2004</td>\n",
       "      <td>cluster1-FLAR19-item-1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ESP</td>\n",
       "      <td>Spain</td>\n",
       "      <td>724</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>17240035017</td>\n",
       "      <td>1.719973e+12</td>\n",
       "      <td>FLA-R-M4-FLA-R-L1-FLA-R-H1</td>\n",
       "      <td>2024-04-11 19:02:08</td>\n",
       "      <td>2024-04-11 19:05:54</td>\n",
       "      <td>en-ZZ</td>\n",
       "      <td>FLARDGSA2004</td>\n",
       "      <td>cluster1-FLAR19-item-1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2008</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ESP</td>\n",
       "      <td>Spain</td>\n",
       "      <td>724</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index        login  last_update_date                testQtiLabel  \\\n",
       "0    0.0  17240191032      1.720307e+12  FLA-R-M4-FLA-R-L1-FLA-R-H1   \n",
       "1    1.0  17240297040      1.720315e+12  FLA-R-M4-FLA-R-L1-FLA-R-H1   \n",
       "2    2.0  17240059030      1.720318e+12  FLA-R-M4-FLA-R-L1-FLA-R-H1   \n",
       "3    3.0  17240291033      1.720319e+12  FLA-R-M4-FLA-R-L1-FLA-R-H1   \n",
       "4    4.0  17240035017      1.719973e+12  FLA-R-M4-FLA-R-L1-FLA-R-H1   \n",
       "\n",
       "      sessionStartTime       sessionEndTime language       unit_id  \\\n",
       "0  2024-05-14 17:40:25  2024-05-14 18:21:43    en-ZZ  FLARDGSA2004   \n",
       "1  2024-04-18 17:32:31  2024-04-18 18:00:44    en-ZZ  FLARDGSA2004   \n",
       "2  2024-04-19 17:48:35  2024-04-19 18:33:31    en-ZZ  FLARDGSA2004   \n",
       "3  2024-05-16 16:53:43  2024-05-16 17:09:31    en-ZZ  FLARDGSA2004   \n",
       "4  2024-04-11 19:02:08  2024-04-11 19:05:54    en-ZZ  FLARDGSA2004   \n",
       "\n",
       "                   itemId score  ...  dob_mm dob_yy sen  mpop1 ppart1  \\\n",
       "0  cluster1-FLAR19-item-1     0  ...       6   2008   0      1      1   \n",
       "1  cluster1-FLAR19-item-1     1  ...      12   2008   0      1      1   \n",
       "2  cluster1-FLAR19-item-1     0  ...       4   2008   0      1      1   \n",
       "3  cluster1-FLAR19-item-1     1  ...       1   2008   0      1      1   \n",
       "4  cluster1-FLAR19-item-1     1  ...       7   2008   3      1      1   \n",
       "\n",
       "  isoalpha3  isoname isocntcd test_attendance questionnaire_attendance  \n",
       "0       ESP    Spain      724             1.0                      1.0  \n",
       "1       ESP    Spain      724             1.0                      1.0  \n",
       "2       ESP    Spain      724             1.0                      1.0  \n",
       "3       ESP    Spain      724             1.0                      1.0  \n",
       "4       ESP    Spain      724             1.0                      1.0  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from test.data_quality.DataQuality import DataQuality\n",
    "from test.utils.utils import create_df_from_dq_results\n",
    "\n",
    "df_check = df_long.loc[df_long['in_cq'] == '1',:]\n",
    "df_check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summ_config = {}\n",
    "\n",
    "conditions = [\n",
    "    df_check['db_score_code'].eq('1'),\n",
    "    df_check['db_score_code'].eq('0'),\n",
    "    df_check['db_score_code'].eq('9'),\n",
    "]\n",
    "codes = [\n",
    "    1,0,0\n",
    "]\n",
    "\n",
    "df_check_sum_score = df_check.copy(deep = True)\n",
    "df_check_sum_score['score_check'] = np.select(conditions,codes,None)\n",
    "\n",
    "\n",
    "df_check_sum_score = df_check_sum_score.groupby(['username','unit_id','score']).agg({'score_check':sum}).reset_index(inplace=False)\n",
    "df_check_sum_score = df_check_sum_score[~df_check_sum_score['unit_id'].isin(cbk.loc[cbk['resp_cat'].str.contains('gap',na=False)].unit_id.unique().tolist())]\n",
    "df_check_sum_score[['score','score_check']] = df_check_sum_score[['score','score_check']].apply(pd.to_numeric)\n",
    "\n",
    "df_summ_config[\"df_check_sum_score\"] = \"config_check_sum_score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table</th>\n",
       "      <th>column</th>\n",
       "      <th>dimension</th>\n",
       "      <th>status</th>\n",
       "      <th>expectation_type</th>\n",
       "      <th>unexpected_count</th>\n",
       "      <th>element_count</th>\n",
       "      <th>unexpected_percent</th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>df_check_sum_score</td>\n",
       "      <td>score; score_check</td>\n",
       "      <td>Validity</td>\n",
       "      <td>PASSED</td>\n",
       "      <td>expect_column_pair_values_to_be_equal</td>\n",
       "      <td>0</td>\n",
       "      <td>424411</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>df_check_sum_score</td>\n",
       "      <td>username</td>\n",
       "      <td>Completeness</td>\n",
       "      <td>PASSED</td>\n",
       "      <td>expect_column_values_to_not_be_null</td>\n",
       "      <td>0</td>\n",
       "      <td>424411</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>df_check_sum_score</td>\n",
       "      <td>username</td>\n",
       "      <td>Completeness</td>\n",
       "      <td>PASSED</td>\n",
       "      <td>expect_column_value_lengths_to_equal</td>\n",
       "      <td>0</td>\n",
       "      <td>424411</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>df_check_sum_score</td>\n",
       "      <td>score</td>\n",
       "      <td>Completeness</td>\n",
       "      <td>PASSED</td>\n",
       "      <td>expect_column_values_to_not_be_null</td>\n",
       "      <td>0</td>\n",
       "      <td>424411</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                table              column     dimension  status  \\\n",
       "0  df_check_sum_score  score; score_check      Validity  PASSED   \n",
       "1  df_check_sum_score            username  Completeness  PASSED   \n",
       "2  df_check_sum_score            username  Completeness  PASSED   \n",
       "3  df_check_sum_score               score  Completeness  PASSED   \n",
       "\n",
       "                        expectation_type  unexpected_count  element_count  \\\n",
       "0  expect_column_pair_values_to_be_equal                 0         424411   \n",
       "1    expect_column_values_to_not_be_null                 0         424411   \n",
       "2   expect_column_value_lengths_to_equal                 0         424411   \n",
       "3    expect_column_values_to_not_be_null                 0         424411   \n",
       "\n",
       "   unexpected_percent  percent  \n",
       "0                 0.0    100.0  \n",
       "1                 0.0    100.0  \n",
       "2                 0.0    100.0  \n",
       "3                 0.0    100.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from test.data_quality.DataQuality import DataQuality\n",
    "from test.utils.utils import create_df_from_dq_results\n",
    "\n",
    "df_summ_tab = []\n",
    "\n",
    "for k, v in df_summ_config.items():\n",
    "    dq = DataQuality(globals()[k],config_path=f\"./test/config/config.json\")\n",
    "    dq_results = dq.run_test()\n",
    "    dq_table = create_df_from_dq_results(dq_results=dq_results).assign(table=k)\n",
    "    cols = dq_table.columns.to_list()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "\n",
    "    df_summ_tab.append(dq_table[cols])\n",
    "\n",
    "dq_table_all = pd.concat(df_summ_tab,axis = 0)\n",
    "dq_table_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A11000035006', 'A12500007032', 'A13000111006', 'A13760082049',\n",
       "       'A13760150001', 'A11580160009'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_vars = cbk.loc[~cbk['resp_cat'].str.startswith('gap',na=False),:].qtiLabel2.to_list()\n",
    "\n",
    "df_sql_check = df_long.loc[\n",
    "    (~pd.isnull(df_long['qtiLabel'])) & (df_long['qtiLabel'].isin(val_vars)) & (~df_long['qtiLabel'].str.endswith('T',na = False)),\n",
    "    ['login','unit_id','itemId','qtiLabel']\n",
    "].assign(dat='1').sort_values(['login','qtiLabel']).merge(\n",
    "    df_long_check[['login','qtiLabel','source']].assign(sql='1'),\n",
    "    how = 'outer',\n",
    "    on = ['login','qtiLabel']\n",
    ")\n",
    "\n",
    "conditions = [\n",
    "    df_sql_check['dat'].eq('1') & df_sql_check['sql'].eq('1'),\n",
    "    df_sql_check['dat'].eq('1') & ~df_sql_check['sql'].eq('1'),\n",
    "    ~df_sql_check['dat'].eq('1') & df_sql_check['sql'].eq('1'),\n",
    "]\n",
    "\n",
    "codes = [\n",
    "    'match',\n",
    "    'dat',\n",
    "    'sql'\n",
    "]\n",
    "\n",
    "df_sql_check['source'] = np.select(conditions,codes,'')\n",
    "df_sql_check.drop(columns = ['sql','dat'],inplace=True)\n",
    "\n",
    "df_sql_check.loc[df_sql_check['source'] != 'match',:].login.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long.loc[df_long['in_cq']=='1',:].groupby(['qtiLabel','score_code','isoalpha3']).size().unstack(fill_value=0).to_excel(f'./data/FLA_freq_Score_byCnt_{datetime.date.today().strftime('%Y%m%d')}.xlsx')\n",
    "df_long.loc[df_long['in_cq']=='1',:].groupby(['qtiLabel','score_code']).size().unstack(fill_value=0).to_excel(f'./data/FLA_freq_Score_Overall_{datetime.date.today().strftime('%Y%m%d')}.xlsx')\n",
    "df_long.loc[df_long['in_cq']=='1',:].groupby(['qtiLabel','cq_cat','isoalpha3']).size().unstack(fill_value=0).to_excel(f'./data/FLA_freq_Resp_byCnt_{datetime.date.today().strftime('%Y%m%d')}.xlsx')\n",
    "df_long.loc[df_long['in_cq']=='1',:].groupby(['qtiLabel','cq_cat']).size().unstack(fill_value=0).to_excel(f'./data/FLA_freq_Resp_Overall_{datetime.date.today().strftime('%Y%m%d')}.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from etl.load import *\n",
    "\n",
    "make_long_file(df_long, domain = 'FLA')\n",
    "make_wide_file(df_long, cbk = cbk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
