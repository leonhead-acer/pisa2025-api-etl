{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare / Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Maple Data and Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "from etl.prepare import *\n",
    "\n",
    "with open('./config.yaml', 'r') as file:\n",
    "    config_data = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redshift_connector\n",
    "from etl.postgresqlschemareader import *\n",
    "\n",
    "with redshift_connector.connect(\n",
    "    host=config_data['ams']['host'],\n",
    "    database=config_data['ams']['database'],\n",
    "    user=config_data['ams']['username'],\n",
    "    password=config_data['ams']['password'],\n",
    "    timeout=999999,\n",
    "    port=5439\n",
    ") as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"\"\"SELECT table_schema, table_name\n",
    "                      FROM information_schema.tables\n",
    "                      WHERE table_schema != 'pg_catalog'\n",
    "                      AND table_schema != 'information_schema'\n",
    "                      AND table_type='BASE TABLE'\n",
    "                      ORDER BY table_schema, table_name\"\"\")\n",
    "\n",
    "        tables = cur.fetchall()\n",
    "        print(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from etl.extract import *\n",
    "from etl.transform import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_num = '1'\n",
    "\n",
    "with postgresql_conn(params = config_data['postgresql_prod']) as conn:\n",
    "    conn.autocommit = True\n",
    "    with conn.cursor() as cur:\n",
    "        # student_participants = extract_student_participants(filepath = '../maple-s3/participants_post.xlsx')\n",
    "        cur.execute(\"SELECT * FROM maple.isot_table\")\n",
    "        isot_table = pd.DataFrame(cur.fetchall()).drop_duplicates('isocntcd',keep = 'last')\n",
    "        cur.execute(\"SELECT * FROM maple.maple_student_post_val\")\n",
    "        student_participants_post = pd.DataFrame(cur.fetchall())\n",
    "        student_participants_post['username'] = student_participants_post['username'].astype(str)\n",
    "        student_participants_post['isocntcd'] = student_participants_post['username'].str.slice(1,4)\n",
    "\n",
    "        countries_all = isot_table.isoalpha3.unique()\n",
    "        countries_now = list(student_participants_post.loc[student_participants_post['batch'] == batch_num,:].isoalpha3.unique())\n",
    "        countries_post = list(student_participants_post.loc[student_participants_post['batch'] != batch_num,:].isoalpha3.unique())\n",
    "        countries_pre_init = list(set(countries_all) - set(countries_now) - set(countries_post))\n",
    "\n",
    "        student_participants_pre = pd.read_excel('../maple-s3/participants_with_entity.xlsx').drop_duplicates(['username'],keep = 'last')\n",
    "        student_participants_pre['username'] = student_participants_pre['username'].astype(str)\n",
    "        student_participants_pre['isocntcd'] = student_participants_pre['username'].str.slice(1,4)\n",
    "        student_participants_pre = student_participants_pre.loc[student_participants_pre['isoalpha3'].isin(countries_pre_init)].drop_duplicates(subset = ['username'],keep = 'last')\n",
    "        student_participants_pre['login'] = student_participants_pre['username']\n",
    "        student_participants_pre = student_participants_pre.rename({'testAttendance':'test_attendance','questionnaireAttendance': 'questionnaire_attendance'},axis = 1)\n",
    "        countries_pre = list(set(student_participants_pre.isoalpha3.unique()) - set(['GBR']))\n",
    "\n",
    "        student_participants = pd.concat(\n",
    "            [\n",
    "                student_participants_pre,\n",
    "                student_participants_post\n",
    "            ],\n",
    "            axis = 0\n",
    "        )\n",
    "        \n",
    "        nc_dat_now = isot_table.loc[isot_table['isoalpha3'].isin(countries_now)].assign(process='now')\n",
    "        nc_dat_post = isot_table.loc[isot_table['isoalpha3'].isin(countries_post)].assign(process='post')\n",
    "        nc_dat_pre = isot_table.loc[isot_table['isoalpha3'].isin(countries_pre)].assign(process='pre')  \n",
    "        nc_dat = pd.concat(\n",
    "            [\n",
    "                nc_dat_post,\n",
    "                nc_dat_now,\n",
    "                nc_dat_pre\n",
    "            ],\n",
    "            axis = 0\n",
    "        )\n",
    "        # nc_dat = extract_country_codes(filepath = './data/maple-s3/ISOT_table.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'etl.load' from 'd:\\\\Users\\\\leon.head\\\\Documents\\\\pisa2025-api-etl\\\\etl\\\\load.py'>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, importlib\n",
    "importlib.reload(sys.modules['etl.load'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting codebook data from sheet FLA_Reading_CQ\n",
      "Extracting codebook data from sheet FLA_Listening_CQ\n",
      "Codebook created for FLA\n",
      "Processing data for: AUT\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: BRN\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: QCY\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Step 1: rows = 898 & columns = 9\n",
      "Step 2: rows = 898 & columns = 33\n",
      "Step 3: rows = 20965 & columns = 8\n",
      "Step 5: rows = 61523 & columns = 22\n",
      "Step 6: rows = 62245 & columns = 50\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_QCY.json: 86.12 seconds\n",
      "Processing data for: DEU\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Step 1: rows = 1240 & columns = 9\n",
      "Step 2: rows = 1240 & columns = 33\n",
      "Step 3: rows = 28950 & columns = 8\n",
      "Step 5: rows = 84868 & columns = 22\n",
      "Step 6: rows = 85921 & columns = 50\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_DEU.json: 195.80 seconds\n",
      "Processing data for: DNK\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Step 1: rows = 885 & columns = 9\n",
      "Step 2: rows = 885 & columns = 33\n",
      "Step 3: rows = 20709 & columns = 8\n",
      "Step 5: rows = 60824 & columns = 22\n",
      "Step 6: rows = 61569 & columns = 50\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_DNK.json: 77.56 seconds\n",
      "Processing data for: QUK\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: QSC\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: HRV\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Step 1: rows = 1569 & columns = 9\n",
      "Step 2: rows = 1569 & columns = 33\n",
      "Step 3: rows = 36644 & columns = 8\n",
      "Step 5: rows = 107138 & columns = 22\n",
      "Step 6: rows = 108468 & columns = 50\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_HRV.json: 174.14 seconds\n",
      "Processing data for: IDN\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: IRL\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: ITA\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Step 1: rows = 1288 & columns = 9\n",
      "Step 2: rows = 1288 & columns = 33\n",
      "Step 3: rows = 30118 & columns = 8\n",
      "Step 5: rows = 88284 & columns = 22\n",
      "Step 6: rows = 89321 & columns = 50\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_ITA.json: 137.35 seconds\n",
      "Processing data for: LTU\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: MAC\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: MNE\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: MNG\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: NLD\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Step 1: rows = 642 & columns = 9\n",
      "Step 2: rows = 642 & columns = 33\n",
      "Step 3: rows = 15023 & columns = 8\n",
      "Step 5: rows = 43594 & columns = 22\n",
      "Step 6: rows = 44147 & columns = 50\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_NLD.json: 59.86 seconds\n",
      "Processing data for: POL\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: PRT\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Step 1: rows = 1458 & columns = 9\n",
      "Step 2: rows = 1458 & columns = 33\n",
      "Step 3: rows = 33934 & columns = 8\n",
      "Step 5: rows = 99496 & columns = 22\n",
      "Step 6: rows = 100694 & columns = 50\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_PRT.json: 230.71 seconds\n",
      "Processing data for: QAT\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Step 1: rows = 1342 & columns = 9\n",
      "Step 2: rows = 1342 & columns = 33\n",
      "Step 3: rows = 31301 & columns = 8\n",
      "Step 5: rows = 91777 & columns = 22\n",
      "Step 6: rows = 92842 & columns = 50\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_QAT.json: 130.48 seconds\n",
      "Processing data for: SAU\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: SGP\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: SRB\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: SVN\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: SWE\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Step 1: rows = 1303 & columns = 9\n",
      "Step 2: rows = 1303 & columns = 33\n",
      "Step 3: rows = 30426 & columns = 8\n",
      "Step 5: rows = 89276 & columns = 22\n",
      "Step 6: rows = 90330 & columns = 50\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_SWE.json: 132.46 seconds\n",
      "Processing data for: TUR\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: URY\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: ALB\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: ARM\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: AUS\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: BGR\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Step 1: rows = 1636 & columns = 9\n",
      "Step 2: rows = 1636 & columns = 33\n",
      "Step 3: rows = 38149 & columns = 8\n",
      "Step 5: rows = 111849 & columns = 22\n",
      "Step 6: rows = 113199 & columns = 50\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_BGR.json: 153.79 seconds\n",
      "Processing data for: BRA\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: CAN\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: CHE\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: COL\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Step 1: rows = 1222 & columns = 9\n",
      "Step 2: rows = 1222 & columns = 33\n",
      "Step 3: rows = 28663 & columns = 8\n",
      "Step 5: rows = 83567 & columns = 22\n",
      "Step 6: rows = 84592 & columns = 50\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_COL.json: 270.21 seconds\n",
      "Processing data for: CRI\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: CZE\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Step 1: rows = 1306 & columns = 9\n",
      "Step 2: rows = 1306 & columns = 33\n",
      "Step 3: rows = 30408 & columns = 8\n",
      "Step 5: rows = 89162 & columns = 22\n",
      "Step 6: rows = 90263 & columns = 50\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_CZE.json: 276.59 seconds\n",
      "Processing data for: ECU\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: ESP\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Step 1: rows = 3198 & columns = 9\n",
      "Step 2: rows = 3198 & columns = 33\n",
      "Step 3: rows = 74552 & columns = 8\n",
      "Step 5: rows = 218790 & columns = 22\n",
      "Step 6: rows = 221399 & columns = 50\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_ESP.json: 549.94 seconds\n",
      "Processing data for: EST\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: FIN\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Step 1: rows = 1236 & columns = 9\n",
      "Step 2: rows = 1236 & columns = 33\n",
      "Step 3: rows = 28820 & columns = 8\n",
      "Step 5: rows = 84324 & columns = 22\n",
      "Step 6: rows = 85331 & columns = 50\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_FIN.json: 198.68 seconds\n",
      "Processing data for: FRA\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Step 1: rows = 1184 & columns = 9\n",
      "Step 2: rows = 1184 & columns = 33\n",
      "Step 3: rows = 27671 & columns = 8\n",
      "Step 5: rows = 81216 & columns = 22\n",
      "Step 6: rows = 82198 & columns = 50\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_FRA.json: 287.77 seconds\n",
      "Processing data for: GEO\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: GRC\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Step 1: rows = 1216 & columns = 9\n",
      "Step 2: rows = 1216 & columns = 33\n",
      "Step 3: rows = 28339 & columns = 8\n",
      "Step 5: rows = 82869 & columns = 22\n",
      "Step 6: rows = 83898 & columns = 50\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_GRC.json: 118.15 seconds\n",
      "Processing data for: HKG\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: HUN\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: ISL\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: ISR\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Step 1: rows = 807 & columns = 9\n",
      "Step 2: rows = 807 & columns = 33\n",
      "Step 3: rows = 18919 & columns = 8\n",
      "Step 5: rows = 55415 & columns = 22\n",
      "Step 6: rows = 56086 & columns = 50\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_ISR.json: 173.65 seconds\n",
      "Processing data for: JOR\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: JPN\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: KAZ\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: KEN\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: KHM\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: KOR\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: LBN\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: LUX\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: LVA\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: MAR\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: MDA\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: MLT\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: MYS\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: NOR\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: NZL\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: PHL\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: PSE\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: ROU\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Step 1: rows = 1380 & columns = 9\n",
      "Step 2: rows = 1380 & columns = 33\n",
      "Step 3: rows = 32202 & columns = 8\n",
      "Step 5: rows = 94452 & columns = 22\n",
      "Step 6: rows = 95569 & columns = 50\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_ROU.json: 333.29 seconds\n",
      "Processing data for: RWA\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: SVK\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: TJK\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: UKR\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Step 1: rows = 1176 & columns = 9\n",
      "Step 2: rows = 1176 & columns = 33\n",
      "Step 3: rows = 27478 & columns = 8\n",
      "Step 5: rows = 80685 & columns = 22\n",
      "Step 6: rows = 81659 & columns = 50\n",
      "Extracting SQL query checks...\n",
      "Time taken for ./data/db/fla/FLA_UKR.json: 104.60 seconds\n",
      "Processing data for: USA\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: UZB\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n",
      "Processing data for: VNM\n",
      "Connection to DB established, searching for new records...\n",
      "Connection to DB closed\n"
     ]
    }
   ],
   "source": [
    "import glob, time\n",
    "\n",
    "count = 0\n",
    "domain_all = [\"FLA\"]\n",
    "\n",
    "for domain in domain_all:\n",
    "    cbk = create_codebook(domain = domain)\n",
    "    \n",
    "    if(domain == 'FLA'):\n",
    "        gap_vars = True\n",
    "\n",
    "    for idr,row in nc_dat.iterrows():\n",
    "        start_time = time.time()\n",
    "        country_print = str(row['isoalpha3'])\n",
    "        print(f\"Processing data for: {country_print}\")\n",
    "\n",
    "        extract_json(domain = domain, nc_dat = row ,overwrite = False, con = postgresql_conn(params = config_data['postgresql_prod']))\n",
    "\n",
    "        filepath = f\"./data/db/{domain.lower()}/{domain}_{country_print}.json\"\n",
    "\n",
    "        if(os.path.isfile(filepath)):\n",
    "            df = read_json_file(filepath)\n",
    "            print(\"Step 1: rows = \" + str(df.shape[0]) + ' & columns = ' + str(df.shape[1]))\n",
    "            \n",
    "            df1 = explode_raw_data(df = df)\n",
    "            print(\"Step 2: rows = \" + str(df1.shape[0]) + ' & columns = ' + str(df1.shape[1]))\n",
    "\n",
    "            df3 = explode_items(df1)\n",
    "            print(\"Step 3: rows = \" + str(df3.shape[0]) + ' & columns = ' + str(df3.shape[1]))\n",
    "\n",
    "            df4 = explode_values(df3)\n",
    "            df4 = rename_variables(df4, domain = domain)\n",
    "            df4 = check_duplicates(df4)\n",
    "            df4 = replace_blank_json(df4)\n",
    "            # print(\"Step 4: rows = \" + str(df4.shape[0]) + ' & columns = ' + str(df4.shape[1]))\n",
    "\n",
    "            df6 = explode_responses(df4, domain = domain)\n",
    "            if(domain == 'FLA'):\n",
    "                df6 = fla_recode_FLALDTB1002(df6)\n",
    "            if(gap_vars):\n",
    "                df6 = gap_recode(df6,cbk)\n",
    "            print(\"Step 5: rows = \" + str(df6.shape[0]) + ' & columns = ' + str(df6.shape[1]))\n",
    "\n",
    "            df8 = merge_cbk_status(df6,cbk,domain = 'FLA')\n",
    "            df8 = time_var_recode(df8)\n",
    "            df8 = score_resp_recode(df8,domain = 'FLA') \n",
    "            df8 = trailing_missing(df8,cbk=cbk)\n",
    "            df8 = cmc_item_create(df8,cbk=cbk, domain = 'FLA')\n",
    "            df9 = merge_participant_info(df8,student_participants=student_participants)\n",
    "            df9['ppart1'] = df9['ppart1'].astype(str).apply(lambda x: re.sub(\".0\",\"\",x)).replace('nan','')\n",
    "            df9['mpop1'] = df9['mpop1'].astype(str).apply(lambda x: re.sub(\".0\",\"\",x)).replace('nan','')\n",
    "            print(\"Step 6: rows = \" + str(df9.shape[0]) + ' & columns = ' + str(df9.shape[1]))\n",
    "\n",
    "            df_resp_check = sql_query_ge(nc_dat = row,cbk = cbk,con = postgresql_conn(params = config_data['postgresql_prod']))\n",
    "\n",
    "            if(count == 0):\n",
    "                df_long = df9\n",
    "                df_long_check = df_resp_check\n",
    "            else:\n",
    "                df_long = pd.concat([df_long,df9],axis = 0)\n",
    "                df_long_check = pd.concat([df_long_check,df_resp_check],axis = 0)\n",
    "            \n",
    "            count =+ 1\n",
    "\n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "            print(f\"Time taken for {filepath}: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "        # df9.export_to_postgresql()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>login</th>\n",
       "      <th>last_update_date</th>\n",
       "      <th>testQtiLabel</th>\n",
       "      <th>sessionStartTime</th>\n",
       "      <th>sessionEndTime</th>\n",
       "      <th>language</th>\n",
       "      <th>unit_id</th>\n",
       "      <th>itemId</th>\n",
       "      <th>score</th>\n",
       "      <th>...</th>\n",
       "      <th>dob_yy</th>\n",
       "      <th>sen</th>\n",
       "      <th>mpop1</th>\n",
       "      <th>ppart1</th>\n",
       "      <th>isocntcd</th>\n",
       "      <th>isoalpha3</th>\n",
       "      <th>isoname</th>\n",
       "      <th>isocntcd</th>\n",
       "      <th>test_attendance</th>\n",
       "      <th>questionnaire_attendance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12.0</td>\n",
       "      <td>11960044002</td>\n",
       "      <td>1.720221e+12</td>\n",
       "      <td>FLA-L-9</td>\n",
       "      <td>2024-04-03 17:19:28</td>\n",
       "      <td>2024-04-03 17:44:21</td>\n",
       "      <td>en-ZZ</td>\n",
       "      <td>FLAL5IMCB1001</td>\n",
       "      <td>cluster1-FLAL09-item-2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>196</td>\n",
       "      <td>QCY</td>\n",
       "      <td>Cyprus</td>\n",
       "      <td>196</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13.0</td>\n",
       "      <td>11960044002</td>\n",
       "      <td>1.720221e+12</td>\n",
       "      <td>FLA-L-9</td>\n",
       "      <td>2024-04-03 17:19:28</td>\n",
       "      <td>2024-04-03 17:44:21</td>\n",
       "      <td>en-ZZ</td>\n",
       "      <td>FLAL5IMCB1001</td>\n",
       "      <td>cluster1-FLAL09-item-2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>196</td>\n",
       "      <td>QCY</td>\n",
       "      <td>Cyprus</td>\n",
       "      <td>196</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15.0</td>\n",
       "      <td>11960044002</td>\n",
       "      <td>1.720221e+12</td>\n",
       "      <td>FLA-L-9</td>\n",
       "      <td>2024-04-03 17:19:28</td>\n",
       "      <td>2024-04-03 17:44:21</td>\n",
       "      <td>en-ZZ</td>\n",
       "      <td>FLAL5IMCB1001</td>\n",
       "      <td>cluster1-FLAL09-item-2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>196</td>\n",
       "      <td>QCY</td>\n",
       "      <td>Cyprus</td>\n",
       "      <td>196</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16.0</td>\n",
       "      <td>11960044002</td>\n",
       "      <td>1.720221e+12</td>\n",
       "      <td>FLA-L-9</td>\n",
       "      <td>2024-04-03 17:19:28</td>\n",
       "      <td>2024-04-03 17:44:21</td>\n",
       "      <td>en-ZZ</td>\n",
       "      <td>FLAL5IMCB1001</td>\n",
       "      <td>cluster1-FLAL09-item-2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>196</td>\n",
       "      <td>QCY</td>\n",
       "      <td>Cyprus</td>\n",
       "      <td>196</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17.0</td>\n",
       "      <td>11960044002</td>\n",
       "      <td>1.720221e+12</td>\n",
       "      <td>FLA-L-9</td>\n",
       "      <td>2024-04-03 17:19:28</td>\n",
       "      <td>2024-04-03 17:44:21</td>\n",
       "      <td>en-ZZ</td>\n",
       "      <td>FLAL5IMCB1001</td>\n",
       "      <td>cluster1-FLAL09-item-2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>196</td>\n",
       "      <td>QCY</td>\n",
       "      <td>Cyprus</td>\n",
       "      <td>196</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index        login  last_update_date testQtiLabel     sessionStartTime  \\\n",
       "12   12.0  11960044002      1.720221e+12      FLA-L-9  2024-04-03 17:19:28   \n",
       "13   13.0  11960044002      1.720221e+12      FLA-L-9  2024-04-03 17:19:28   \n",
       "15   15.0  11960044002      1.720221e+12      FLA-L-9  2024-04-03 17:19:28   \n",
       "16   16.0  11960044002      1.720221e+12      FLA-L-9  2024-04-03 17:19:28   \n",
       "17   17.0  11960044002      1.720221e+12      FLA-L-9  2024-04-03 17:19:28   \n",
       "\n",
       "         sessionEndTime language        unit_id                  itemId score  \\\n",
       "12  2024-04-03 17:44:21    en-ZZ  FLAL5IMCB1001  cluster1-FLAL09-item-2     1   \n",
       "13  2024-04-03 17:44:21    en-ZZ  FLAL5IMCB1001  cluster1-FLAL09-item-2     1   \n",
       "15  2024-04-03 17:44:21    en-ZZ  FLAL5IMCB1001  cluster1-FLAL09-item-2     1   \n",
       "16  2024-04-03 17:44:21    en-ZZ  FLAL5IMCB1001  cluster1-FLAL09-item-2     1   \n",
       "17  2024-04-03 17:44:21    en-ZZ  FLAL5IMCB1001  cluster1-FLAL09-item-2     1   \n",
       "\n",
       "    ...  dob_yy sen mpop1  ppart1 isocntcd isoalpha3  isoname isocntcd  \\\n",
       "12  ...    2008   0     1       1      196       QCY   Cyprus      196   \n",
       "13  ...    2008   0     1       1      196       QCY   Cyprus      196   \n",
       "15  ...    2008   0     1       1      196       QCY   Cyprus      196   \n",
       "16  ...    2008   0     1       1      196       QCY   Cyprus      196   \n",
       "17  ...    2008   0     1       1      196       QCY   Cyprus      196   \n",
       "\n",
       "   test_attendance questionnaire_attendance  \n",
       "12               1                        1  \n",
       "13               1                        1  \n",
       "15               1                        1  \n",
       "16               1                        1  \n",
       "17               1                        1  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from test.data_quality.DataQuality import DataQuality\n",
    "from test.utils.utils import create_df_from_dq_results\n",
    "\n",
    "df_check = df_long.loc[df_long['in_cq'] == '1',:]\n",
    "df_check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summ_config = {}\n",
    "\n",
    "conditions = [\n",
    "    df_check['db_score_code'].eq('1'),\n",
    "    df_check['db_score_code'].eq('0'),\n",
    "    df_check['db_score_code'].eq('9'),\n",
    "]\n",
    "codes = [\n",
    "    1,0,0\n",
    "]\n",
    "\n",
    "df_check_sum_score = df_check.copy(deep = True)\n",
    "df_check_sum_score['score_check'] = np.select(conditions,codes,None)\n",
    "\n",
    "df_check_sum_score = df_check_sum_score.groupby(['username','unit_id','score']).agg({'score_check':sum}).reset_index(inplace=False)\n",
    "df_check_sum_score = df_check_sum_score[~df_check_sum_score['unit_id'].isin(cbk.loc[cbk['resp_cat'].str.contains('gap',na=False)].unit_id.unique().tolist())]\n",
    "df_check_sum_score[['score','score_check']] = df_check_sum_score[['score','score_check']].apply(pd.to_numeric)\n",
    "\n",
    "df_summ_config[\"df_check_sum_score\"] = \"config_check_sum_score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table</th>\n",
       "      <th>column</th>\n",
       "      <th>dimension</th>\n",
       "      <th>status</th>\n",
       "      <th>expectation_type</th>\n",
       "      <th>unexpected_count</th>\n",
       "      <th>element_count</th>\n",
       "      <th>unexpected_percent</th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>df_check_sum_score</td>\n",
       "      <td>score; score_check</td>\n",
       "      <td>Validity</td>\n",
       "      <td>PASSED</td>\n",
       "      <td>expect_column_pair_values_to_be_equal</td>\n",
       "      <td>0</td>\n",
       "      <td>388205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>df_check_sum_score</td>\n",
       "      <td>username</td>\n",
       "      <td>Completeness</td>\n",
       "      <td>PASSED</td>\n",
       "      <td>expect_column_values_to_not_be_null</td>\n",
       "      <td>0</td>\n",
       "      <td>388205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>df_check_sum_score</td>\n",
       "      <td>username</td>\n",
       "      <td>Completeness</td>\n",
       "      <td>PASSED</td>\n",
       "      <td>expect_column_value_lengths_to_equal</td>\n",
       "      <td>0</td>\n",
       "      <td>388205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>df_check_sum_score</td>\n",
       "      <td>score</td>\n",
       "      <td>Completeness</td>\n",
       "      <td>PASSED</td>\n",
       "      <td>expect_column_values_to_not_be_null</td>\n",
       "      <td>0</td>\n",
       "      <td>388205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                table              column     dimension  status  \\\n",
       "0  df_check_sum_score  score; score_check      Validity  PASSED   \n",
       "1  df_check_sum_score            username  Completeness  PASSED   \n",
       "2  df_check_sum_score            username  Completeness  PASSED   \n",
       "3  df_check_sum_score               score  Completeness  PASSED   \n",
       "\n",
       "                        expectation_type  unexpected_count  element_count  \\\n",
       "0  expect_column_pair_values_to_be_equal                 0         388205   \n",
       "1    expect_column_values_to_not_be_null                 0         388205   \n",
       "2   expect_column_value_lengths_to_equal                 0         388205   \n",
       "3    expect_column_values_to_not_be_null                 0         388205   \n",
       "\n",
       "   unexpected_percent  percent  \n",
       "0                 0.0    100.0  \n",
       "1                 0.0    100.0  \n",
       "2                 0.0    100.0  \n",
       "3                 0.0    100.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from test.data_quality.DataQuality import DataQuality\n",
    "from test.utils.utils import create_df_from_dq_results\n",
    "\n",
    "df_summ_tab = []\n",
    "\n",
    "for k, v in df_summ_config.items():\n",
    "    dq = DataQuality(globals()[k],config_path=f\"./test/config/config.json\")\n",
    "    dq_results = dq.run_test()\n",
    "    dq_table = create_df_from_dq_results(dq_results=dq_results).assign(table=k)\n",
    "    cols = dq_table.columns.to_list()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "\n",
    "    df_summ_tab.append(dq_table[cols])\n",
    "\n",
    "dq_table_all = pd.concat(df_summ_tab,axis = 0)\n",
    "dq_table_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A11000035006', 'A12500007032', 'A13000111006', 'A13760082049',\n",
       "       'A13760150001'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_vars = cbk.loc[~cbk['resp_cat'].str.startswith('gap',na=False),:].qtiLabel2.to_list()\n",
    "\n",
    "df_sql_check = df_long.loc[\n",
    "    (~pd.isnull(df_long['qtiLabel'])) & (df_long['qtiLabel'].isin(val_vars)) & (~df_long['qtiLabel'].str.endswith('T',na = False)),\n",
    "    ['login','unit_id','itemId','qtiLabel']\n",
    "].assign(dat='1').sort_values(['login','qtiLabel']).merge(\n",
    "    df_long_check[['login','qtiLabel','source']].assign(sql='1'),\n",
    "    how = 'outer',\n",
    "    on = ['login','qtiLabel']\n",
    ")\n",
    "\n",
    "conditions = [\n",
    "    df_sql_check['dat'].eq('1') & df_sql_check['sql'].eq('1'),\n",
    "    df_sql_check['dat'].eq('1') & ~df_sql_check['sql'].eq('1'),\n",
    "    ~df_sql_check['dat'].eq('1') & df_sql_check['sql'].eq('1'),\n",
    "]\n",
    "\n",
    "codes = [\n",
    "    'match',\n",
    "    'dat',\n",
    "    'sql'\n",
    "]\n",
    "\n",
    "df_sql_check['source'] = np.select(conditions,codes,'')\n",
    "df_sql_check.drop(columns = ['sql','dat'],inplace=True)\n",
    "\n",
    "df_sql_check.loc[df_sql_check['source'] != 'match',:].login.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long.loc[df_long['in_cq']=='1',:].groupby(['qtiLabel','score_code','isoalpha3']).size().unstack(fill_value=0).to_excel('./data/FLA_freq_Score_byCnt.xlsx')\n",
    "df_long.loc[df_long['in_cq']=='1',:].groupby(['qtiLabel','score_code']).size().unstack(fill_value=0).to_excel('./data/FLA_freq_Score_Overall.xlsx')\n",
    "df_long.loc[df_long['in_cq']=='1',:].groupby(['qtiLabel','cq_cat','isoalpha3']).size().unstack(fill_value=0).to_excel('./data/FLA_freq_Resp_byCnt.xlsx')\n",
    "df_long.loc[df_long['in_cq']=='1',:].groupby(['qtiLabel','cq_cat']).size().unstack(fill_value=0).to_excel('./data/FLA_freq_Resp_Overall.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_longx = df_long.copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "df_longx['ppart1'] = df_longx['ppart1'].astype(str).apply(lambda x: re.sub(\".0\",\"\",x)).replace('nan','')\n",
    "df_longx['mpop1'] = df_longx['mpop1'].astype(str).apply(lambda x: re.sub(\".0\",\"\",x)).replace('nan','')\n",
    "df_longx = df_longx.loc[:,~df_longx.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from etl.load import *\n",
    "\n",
    "make_long_file(df_longx, domain = 'FLA')\n",
    "make_wide_file(df_longx, cbk = cbk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
